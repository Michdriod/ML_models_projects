{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: manual_test_sample_raw.csv\n",
      "\n",
      "✅ All saved models, scalers, and files have been deleted!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the file types to delete (adjust extensions if needed)\n",
    "files_to_delete = [\"*.pkl\", \"*.csv\", \"*.xlsx\"]  # Adjust as needed\n",
    "\n",
    "# Loop through each file type and delete them\n",
    "for file_type in files_to_delete:\n",
    "    for file in glob.glob(file_type):  # Find all matching files\n",
    "        try:\n",
    "            os.remove(file)  # Delete the file\n",
    "            print(f\"Deleted: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {e}\")\n",
    "\n",
    "print(\"\\n✅ All saved models, scalers, and files have been deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model Training & Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# Model Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Ensemble Learning\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# analyze the impact of each feature on the model's decision-making process.\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Explore the Dataset (Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "credit_card_fraud = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Downloads\\\\creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "\n",
    "credit_card_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the last few rows\n",
    "\n",
    "credit_card_fraud.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset shape\n",
    "\n",
    "credit_card_fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution\n",
    "\n",
    "credit_card_fraud[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# exploring the datatype of each column\n",
    "\n",
    "credit_card_fraud.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "credit_card_fraud.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check basic statistics of the dataset\n",
    "\n",
    "credit_card_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance Ratio: 577.8760\n"
     ]
    }
   ],
   "source": [
    "# imbalance ratio of fruad cases and normal cases\n",
    "\n",
    "fraud_cases = credit_card_fraud[credit_card_fraud[\"Class\"] == 1]\n",
    "normal_cases = credit_card_fraud[credit_card_fraud[\"Class\"] == 0]\n",
    "\n",
    "imbalance_ratio = len(normal_cases) / len(fraud_cases)\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Features & Target\n",
    "\n",
    "We separate the features from the target variable and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10 records from the test set (before scaling)\n",
    "sample_indices = X_test.sample(10, random_state=42).index\n",
    "X_manual_sample = X_test.loc[sample_indices]\n",
    "y_manual_sample = y_test.loc[sample_indices]\n",
    "\n",
    "# Save the original (unscaled) test samples for manual checking\n",
    "X_manual_sample.to_csv(\"manual_test_sample_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Features:\n",
      "            Time         V1         V2         V3        V4         V5  \\\n",
      "197288  131941.0   1.427126  -0.919876  -3.165507  1.378848   0.867670   \n",
      "175745  122469.0   1.808834  -1.333030  -0.556548 -0.165515  -1.388384   \n",
      "29594    35541.0   1.028807  -0.167460   1.344620  1.649794  -0.717662   \n",
      "195743  131201.0  -1.824223  -0.620021  -0.467199 -0.407000  -0.476089   \n",
      "234355  147939.0   1.984241   0.115783  -1.659127  0.440728   0.174926   \n",
      "201844  134059.0  -3.146311   1.598155  -0.963120 -0.404630  -0.100386   \n",
      "5560      5680.0  -0.348230   0.397211   2.349409  1.173396  -0.133879   \n",
      "170199  120066.0  -7.880589   6.550019  -6.172591 -0.769679  -4.553408   \n",
      "98802    66824.0   1.185228   0.363215   0.413729  1.251028  -0.243935   \n",
      "267610  162849.0   2.038329  -0.114510  -1.210975  0.198567   0.147862   \n",
      "104081   68905.0   1.174641  -0.084897  -0.256067 -0.219802  -0.149242   \n",
      "279009  168586.0  -0.496714   2.008929  -2.410261 -0.260957   2.014841   \n",
      "180171  124413.0  -0.107283   1.061253  -1.098503 -1.474405   1.664497   \n",
      "29290    35398.0   1.051470  -0.082306   1.378119  1.221591  -0.787408   \n",
      "234405  147962.0  -4.251648  -2.801520  -0.713997  0.403558   1.977649   \n",
      "116757   74416.0  -0.464374  -0.202580   2.092739 -2.728698  -0.546591   \n",
      "67250    52433.0   1.214687  -0.333027   1.066594 -0.042089  -0.961937   \n",
      "129615   79157.0   1.318065  -0.613459   0.304633 -0.421477  -0.986629   \n",
      "266625  162419.0   2.094166  -0.226142  -1.524207  0.059664   0.322458   \n",
      "120365   75803.0   1.104021   0.068399  -0.040738  0.860546  -0.040032   \n",
      "29325    35422.0   0.448093  -0.922810  -0.013293  1.564040  -0.734068   \n",
      "145931   87293.0  -0.527826   1.038012   0.870553 -1.034371   0.980892   \n",
      "106165   69851.0   1.332262   0.515372  -1.007029  0.398491   0.777826   \n",
      "45791    42497.0   1.286209  -0.824774   0.829653 -0.858971  -1.142345   \n",
      "166282  117973.0   1.892133  -0.371775  -0.016741  0.477976  -0.952702   \n",
      "43493    41527.0  -2.457335   1.776473  -1.887868  1.117323  -1.247361   \n",
      "231476  146778.0  -3.382037   2.043214   1.699968 -1.275629  -1.374436   \n",
      "156151  107425.0  -0.007532   0.569647   1.406002 -0.715873   0.485044   \n",
      "45188    42241.0  -0.856166  -0.446218   0.793857 -1.642759   1.452159   \n",
      "31905    36556.0   1.287944   0.807170  -0.208934  2.279783   0.704599   \n",
      "42528    41138.0  -4.595617   5.083690  -7.581015  7.546033  -6.949165   \n",
      "149577   91502.0   0.007379   2.365183  -2.600287  1.111602   3.276441   \n",
      "119714   75556.0  -0.734303   0.435519  -0.530866 -0.471120   0.643214   \n",
      "152019   96717.0  -3.705856   4.107873  -3.803656  1.710314  -3.582466   \n",
      "150663   93853.0  -5.839192   7.151532 -12.816760  7.031115  -9.651272   \n",
      "106998   70229.0   0.315642   1.636778  -1.519650  4.028571  -1.186794   \n",
      "105178   69394.0   1.140431   1.134243  -1.429455  2.012226   0.622800   \n",
      "64411    51135.0 -10.527304   7.639745 -13.443115  4.303403  -8.048210   \n",
      "70589    53937.0  -2.042608   1.573578  -2.372652 -0.572676  -2.097353   \n",
      "77348    57007.0  -1.271244   2.462675  -2.851395  2.324480  -1.372245   \n",
      "6108      6986.0  -4.397974   1.358367  -2.592844  2.679787  -1.128131   \n",
      "251904  155554.0  -1.040067   3.106703  -5.409027  3.109903  -0.887237   \n",
      "102444   68207.0 -13.192671  12.785971  -9.906650  3.320337  -4.801176   \n",
      "30496    35953.0  -4.844372   5.649439  -6.730396  5.252842  -4.409566   \n",
      "41569    40742.0  -2.377533   0.520539  -8.094139  8.005351   2.640750   \n",
      "42958    41313.0 -13.897206   6.344280 -14.281666  5.581009 -12.887133   \n",
      "18809    29785.0   0.923764   0.344048  -2.880004  1.721680  -3.019565   \n",
      "222133  142840.0  -3.613850  -0.922136  -4.749887  3.373001  -0.545207   \n",
      "157918  110617.0  -1.101035  -1.674928  -0.573388  5.617556   0.765556   \n",
      "241445  151029.0  -3.818214   2.551338  -4.759158  1.636967  -1.167900   \n",
      "81186    58822.0  -4.384221   3.264665  -3.077158  3.403594  -1.938075   \n",
      "149869   92092.0  -1.108478   3.448953  -6.216972  3.021052  -0.529901   \n",
      "111690   72327.0  -4.198735   0.194121  -3.917586  3.920748  -1.875486   \n",
      "229712  146022.0   0.908637   2.849024  -5.647343  6.009415   0.216656   \n",
      "68067    52814.0  -1.101847  -1.632441   0.901067  0.847753  -1.249091   \n",
      "142405   84694.0  -4.868108   1.264420  -5.167885  3.193648  -3.045621   \n",
      "141260   84204.0  -1.927453   1.827621  -7.019495  5.348303  -2.739188   \n",
      "218442  141320.0  -6.352337  -2.370335  -4.875397  2.335045  -0.809555   \n",
      "14170    25198.0 -15.903635  10.393917 -19.133602  6.185969 -12.538021   \n",
      "77682    57163.0 -10.363049   4.543672  -9.795898  5.508003  -6.037156   \n",
      "\n",
      "              V6         V7         V8        V9  ...       V20        V21  \\\n",
      "197288 -0.880074   1.482888  -0.569838 -0.018398  ...  0.395290   0.453192   \n",
      "175745 -0.731744  -0.663268  -0.092321  0.479492  ... -0.424837  -0.503476   \n",
      "29594   0.807931  -0.610833   0.397065  0.923157  ... -0.188325  -0.348086   \n",
      "195743  1.391575   2.452366   0.441657 -0.843092  ...  0.952882   0.284532   \n",
      "234355 -1.167454   0.153436  -0.217630  0.358445  ... -0.203359   0.260785   \n",
      "201844  0.472588  -0.754145  -2.361922 -0.226883  ... -1.341400   3.058008   \n",
      "5560    0.454458  -0.226819   0.054197  1.735789  ... -0.050007  -0.139747   \n",
      "170199 -2.077868  -3.887652   5.692164  0.419513  ...  0.294035  -0.144865   \n",
      "98802  -0.836648   0.250600  -0.277617 -0.207836  ... -0.008214   0.100373   \n",
      "267610 -0.573655   0.064942  -0.141620  0.294074  ... -0.200556  -0.256774   \n",
      "104081 -0.888968   0.348767  -0.191992 -0.257254  ...  0.057121  -0.318735   \n",
      "279009 -1.070308   1.568992  -0.616792 -0.182306  ... -0.113325   0.209197   \n",
      "180171 -0.850369   1.719095  -0.445303 -0.093699  ...  0.092863   0.131645   \n",
      "29290   0.485088  -0.693685   0.327520  0.522859  ... -0.121883   0.037766   \n",
      "234405 -2.065485   0.796150  -0.458803  0.518710  ... -2.078305  -0.880910   \n",
      "116757 -0.232174  -0.125060  -0.253106 -2.189879  ...  0.042412  -0.144450   \n",
      "67250   0.059843  -0.785062   0.190605  0.794023  ... -0.033185  -0.088958   \n",
      "129615 -0.854191  -0.342662  -0.262914 -0.626589  ...  0.179261   0.087558   \n",
      "266625 -0.302063  -0.030850  -0.043578  0.556196  ... -0.262701  -0.323709   \n",
      "120365  0.175008  -0.262475   0.219732  0.392851  ... -0.068815  -0.115750   \n",
      "29325  -0.760531   0.699543  -0.282068 -0.055189  ...  0.660541   0.065627   \n",
      "145931 -0.317789   1.320843  -0.296049 -0.469022  ...  0.145068  -0.272577   \n",
      "106165 -0.308176   0.266058  -0.084887 -0.329550  ... -0.020428  -0.175476   \n",
      "45791   0.251987  -1.213331   0.254026 -0.600934  ...  0.090627   0.444081   \n",
      "166282 -0.677348  -0.590229   0.075704  1.058846  ... -0.273593  -0.091779   \n",
      "43493  -0.875904  -0.314289   1.602283 -0.370446  ... -0.207412   0.178758   \n",
      "231476  3.160482  -5.166279 -10.466988  1.473967  ...  1.342748  -0.796266   \n",
      "156151  0.003756   0.615303  -0.457998  2.017190  ... -0.087583  -0.219834   \n",
      "45188  -0.857706   0.318992  -0.264565 -1.516346  ...  0.482476   0.147492   \n",
      "31905  -0.342407   0.622447  -0.291085 -1.158905  ... -0.069512  -0.057532   \n",
      "42528  -1.729185  -8.190192   2.714670 -7.083169  ...  1.682160   2.248971   \n",
      "149577 -1.776141   2.114531  -0.830084  0.900490  ... -0.006388  -0.563944   \n",
      "119714  0.713832  -1.234572  -2.551412 -2.057724  ...  0.864536  -1.004877   \n",
      "152019  1.469729  -9.621560 -11.913105 -0.322297  ...  3.639603  -5.498772   \n",
      "150663 -2.938427 -11.543207   4.843627 -3.494276  ...  0.055684   2.462056   \n",
      "106998 -0.789813  -2.279807   0.472988 -1.657635  ...  0.388885   0.345921   \n",
      "105178 -1.152923   0.221159   0.037372  0.034486  ... -0.099712  -0.367136   \n",
      "64411  -3.466997  -8.643193   7.284105 -2.362097  ...  0.847085   0.937416   \n",
      "70589  -0.174142  -3.039520  -1.634233 -0.594809  ...  0.825566  -0.723326   \n",
      "77348  -0.948196  -3.065234   1.166927 -2.268771  ...  0.560478   0.652941   \n",
      "6108   -1.706536  -3.496197  -0.248778 -0.247768  ... -0.171608   0.573574   \n",
      "251904 -2.497522  -2.073347   0.639818 -3.013331  ...  1.054390   0.773961   \n",
      "102444  5.760059 -18.750889 -37.353443 -0.391540  ... -3.493050  27.202839   \n",
      "30496  -1.740767  -6.311699   3.449167 -5.416284  ...  0.284555   1.194888   \n",
      "41569  -3.381586  -1.934372   0.562322 -3.104027  ... -0.634747   0.148284   \n",
      "42958  -3.146176 -15.450467   9.060281 -5.486121  ... -1.025228   3.058082   \n",
      "18809  -0.639736  -3.801325   1.299096  0.864065  ...  0.170872   0.899931   \n",
      "222133 -1.171301  -4.172315   1.517016 -1.775833  ... -0.320541   0.786787   \n",
      "157918  0.440607   1.934740  -1.019788 -0.193244  ... -0.038314  -0.412526   \n",
      "241445 -1.678413  -3.144732   1.245106 -1.692541  ...  0.164453   0.837685   \n",
      "81186  -1.221081  -3.310317  -1.111975 -1.977593  ... -0.141533   2.076383   \n",
      "149869 -2.551375  -2.001743   1.092432 -0.836098  ... -0.068598   0.825951   \n",
      "111690 -2.118933  -3.614445   1.687884 -2.189871  ...  1.003350   0.801312   \n",
      "229712 -2.397014  -1.819308   0.338527 -2.819883  ...  0.241921   0.407260   \n",
      "68067   0.654937   1.448868   0.023308 -0.136742  ...  1.230278   0.610654   \n",
      "142405 -2.096166  -6.445610   2.422536 -3.214055  ...  0.667310   1.269205   \n",
      "141260 -2.107219  -5.015848   1.205868 -4.382713  ...  2.172709   1.376938   \n",
      "218442 -0.413647  -4.082308   2.239089 -1.986360  ...  0.186898   1.325218   \n",
      "14170  -4.027030 -13.897827  10.662252 -2.844954  ...  1.501565   1.577548   \n",
      "77682  -0.133493 -11.724346  -3.198346 -4.767842  ...  0.924396  -2.457145   \n",
      "\n",
      "             V22       V23       V24       V25       V26       V27       V28  \\\n",
      "197288  0.580948 -0.664473 -1.036310  0.811282 -0.162522 -0.134535 -0.026835   \n",
      "175745 -1.234127  0.331022 -0.161604 -0.565929 -0.989281  0.043581  0.000009   \n",
      "29594  -0.536813  0.095209  0.228722  0.381645 -0.540081  0.090843  0.020739   \n",
      "195743 -0.127787  1.247207 -1.153693 -0.066464  0.361555 -0.209685  0.060636   \n",
      "234355  0.869906 -0.019846 -0.051755  0.186102 -0.121079 -0.005895 -0.040131   \n",
      "201844 -0.910790  0.137177  0.633245  1.076356  0.732340 -0.392932 -0.388537   \n",
      "5560    0.105727 -0.144609 -0.068840 -0.374898 -0.455427 -0.180041 -0.161515   \n",
      "170199 -1.034693  0.828411 -0.017477  0.514839  0.199278 -0.200415 -0.164711   \n",
      "98802   0.342842 -0.146701  0.437887  0.707106 -0.288043  0.021624  0.029314   \n",
      "267610 -0.629232  0.283940 -0.463624 -0.287351  0.208021 -0.071350 -0.074496   \n",
      "104081 -1.144258  0.044904  0.036720  0.185374  0.733109 -0.127805 -0.004269   \n",
      "279009  0.392837 -0.199179 -0.001653 -0.754553  0.496729 -0.765836  0.292676   \n",
      "180171  0.760815 -0.420303 -1.075280 -0.207009  0.115318  0.327715  0.081353   \n",
      "29290   0.341339  0.029649  0.238487  0.304656 -0.411124  0.089330  0.028097   \n",
      "234405  0.186535  2.502266 -0.051092 -0.068930  0.253335  0.262927  0.134776   \n",
      "116757  0.264018 -0.442479 -0.435246  0.558927 -0.151099  0.155027 -0.098123   \n",
      "67250  -0.057786  0.009784  0.097121  0.143736  0.992647 -0.034060  0.002272   \n",
      "129615  0.293528 -0.152476  0.457701  0.695271 -0.153009  0.002000  0.017874   \n",
      "266625 -0.894058  0.209918 -1.134310 -0.229122  0.249894 -0.084651 -0.083882   \n",
      "120365 -0.144651 -0.130336 -0.376624  0.471390  0.443267  0.008397  0.028442   \n",
      "29325  -0.740807 -0.304282  0.372358  0.306718 -0.578691 -0.061109  0.096178   \n",
      "145931 -0.497694 -0.234497 -0.443899 -0.066822  0.188651 -0.012443 -0.064900   \n",
      "106165 -0.477544 -0.259984 -1.116594  0.723588  0.435491 -0.039408  0.012735   \n",
      "45791   1.195210 -0.133939 -0.258745  0.354588 -0.017627  0.045587  0.010740   \n",
      "166282 -0.168484  0.441567  0.430369 -0.565386 -0.669298  0.031838 -0.036802   \n",
      "43493   0.408755  0.013528  0.069940 -0.653713 -0.366042  0.399220 -0.010572   \n",
      "231476  0.535592  0.450395  0.574513  0.187076  1.378020  0.131999  0.037386   \n",
      "156151 -0.070806 -0.019413 -0.506429 -0.702553 -1.116036 -0.488843 -0.394033   \n",
      "45188   0.236355 -0.285595 -0.801895  0.844480 -0.172266 -0.237157 -0.234738   \n",
      "31905  -0.129684 -0.221658 -0.427543  0.877594  0.117169 -0.033912  0.003912   \n",
      "42528   0.566844  0.033744  0.591783  0.334229  0.386801  2.163898  0.983104   \n",
      "149577 -0.902100 -0.404382 -0.012944  0.589836 -0.734449 -0.447529 -0.362375   \n",
      "119714  1.150354 -0.152555 -1.386745  0.004716  0.219146 -0.058257  0.158048   \n",
      "152019  2.941475  0.916236 -0.255504 -0.183835 -0.584539 -0.315484 -0.097223   \n",
      "150663  1.054865  0.530481  0.472670 -0.275998  0.282435  0.104886  0.254417   \n",
      "106998 -0.108002 -0.165442  0.279895  0.808783  0.117363  0.589595  0.309064   \n",
      "105178 -0.891627 -0.160578 -0.108326  0.668374 -0.352393  0.071993  0.113684   \n",
      "64411  -0.931178 -0.235697 -0.031393  0.591558 -0.263516  1.108897  0.219021   \n",
      "70589   0.501222 -0.696892 -0.600514  0.127547 -0.786072  0.606097  0.171697   \n",
      "77348   0.081931 -0.221348 -0.523582  0.224228  0.756335  0.632800  0.250187   \n",
      "6108    0.176968 -0.436207 -0.053502  0.252405 -0.657488 -0.827136  0.849573   \n",
      "251904  0.214868 -0.184233 -0.284091  0.493467  0.732329  0.675067  0.337076   \n",
      "102444 -8.887017  5.303607 -0.639435  0.263203 -0.108877  1.269566  0.939407   \n",
      "30496  -0.845753  0.190674 -0.216443 -0.325033 -0.270328  0.210214  0.391855   \n",
      "41569   0.721100  2.661291 -0.508620 -0.401657  0.587611  0.500326  0.551760   \n",
      "42958   0.941180 -0.232710  0.763508  0.075456 -0.453840 -1.508968 -0.686836   \n",
      "18809   1.481271  0.725266  0.176960 -1.815638 -0.536517  0.489035 -0.049729   \n",
      "222133  0.893065  1.034907  0.097671 -1.345551 -0.788329  1.055442  0.099971   \n",
      "157918 -0.208823  0.344833  1.091435 -0.686513  0.075809  0.033865 -0.832855   \n",
      "241445  0.761712 -0.417694 -0.469712 -0.225934  0.586415 -0.348107  0.087777   \n",
      "81186  -0.990303 -0.330358  0.158378  0.006351 -0.493860 -1.537652 -0.994022   \n",
      "149869  1.144170  0.208559 -0.295497 -0.690232 -0.364749  0.229327  0.208830   \n",
      "111690 -0.183001 -0.440387  0.292539 -0.144967 -0.251744  1.249414 -0.131525   \n",
      "229712 -0.397435 -0.080006 -0.168597  0.465058  0.210510  0.648705  0.360224   \n",
      "68067   0.835795  1.179955 -0.029091 -0.300896  0.699175 -0.336072 -0.177587   \n",
      "142405  0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274 -0.127944   \n",
      "141260 -0.792017 -0.771414 -0.379574  0.718717  1.111151  1.277707  0.819081   \n",
      "218442  1.226745 -1.485217 -1.470732 -0.240053  0.112972  0.910591 -0.650944   \n",
      "14170  -1.280137 -0.601295  0.040404  0.995502 -0.273743  1.688136  0.527831   \n",
      "77682   1.687257  0.977178 -0.543369 -0.289125 -0.107586  0.330642  0.163577   \n",
      "\n",
      "        Amount  \n",
      "197288  385.00  \n",
      "175745  158.94  \n",
      "29594     7.40  \n",
      "195743  556.16  \n",
      "234355   11.30  \n",
      "201844   39.93  \n",
      "5560     12.99  \n",
      "170199    8.99  \n",
      "98802    29.99  \n",
      "267610    1.98  \n",
      "104081   69.95  \n",
      "279009    0.76  \n",
      "180171    0.77  \n",
      "29290     9.99  \n",
      "234405   55.41  \n",
      "116757   11.80  \n",
      "67250     0.00  \n",
      "129615   55.00  \n",
      "266625    1.98  \n",
      "120365   29.56  \n",
      "29325   409.25  \n",
      "145931    5.37  \n",
      "106165    0.76  \n",
      "45791    24.99  \n",
      "166282    7.50  \n",
      "43493    99.99  \n",
      "231476   10.00  \n",
      "156151   11.27  \n",
      "45188    36.88  \n",
      "31905     3.62  \n",
      "42528   340.11  \n",
      "149577    1.00  \n",
      "119714   29.95  \n",
      "152019    1.00  \n",
      "150663  316.06  \n",
      "106998    3.79  \n",
      "105178    1.00  \n",
      "64411    99.99  \n",
      "70589   261.87  \n",
      "77348     0.01  \n",
      "6108     59.00  \n",
      "251904   94.82  \n",
      "102444    1.00  \n",
      "30496   111.70  \n",
      "41569     1.00  \n",
      "42958     9.99  \n",
      "18809    30.30  \n",
      "222133  144.80  \n",
      "157918  635.10  \n",
      "241445   10.70  \n",
      "81186    45.64  \n",
      "149869   18.00  \n",
      "111690  238.90  \n",
      "229712    1.18  \n",
      "68067   519.90  \n",
      "142405   12.31  \n",
      "141260  512.25  \n",
      "218442  195.66  \n",
      "14170    99.99  \n",
      "77682     1.00  \n",
      "\n",
      "[60 rows x 30 columns]\n",
      "\n",
      "Sampled Labels:\n",
      "197288    0\n",
      "175745    0\n",
      "29594     0\n",
      "195743    0\n",
      "234355    0\n",
      "201844    0\n",
      "5560      0\n",
      "170199    0\n",
      "98802     0\n",
      "267610    0\n",
      "104081    0\n",
      "279009    0\n",
      "180171    0\n",
      "29290     0\n",
      "234405    0\n",
      "116757    0\n",
      "67250     0\n",
      "129615    0\n",
      "266625    0\n",
      "120365    0\n",
      "29325     0\n",
      "145931    0\n",
      "106165    0\n",
      "45791     0\n",
      "166282    0\n",
      "43493     0\n",
      "231476    0\n",
      "156151    0\n",
      "45188     0\n",
      "31905     0\n",
      "42528     1\n",
      "149577    1\n",
      "119714    1\n",
      "152019    1\n",
      "150663    1\n",
      "106998    1\n",
      "105178    1\n",
      "64411     1\n",
      "70589     1\n",
      "77348     1\n",
      "6108      1\n",
      "251904    1\n",
      "102444    1\n",
      "30496     1\n",
      "41569     1\n",
      "42958     1\n",
      "18809     1\n",
      "222133    1\n",
      "157918    1\n",
      "241445    1\n",
      "81186     1\n",
      "149869    1\n",
      "111690    1\n",
      "229712    1\n",
      "68067     1\n",
      "142405    1\n",
      "141260    1\n",
      "218442    1\n",
      "14170     1\n",
      "77682     1\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample 5 records from Class 0\n",
    "class_0_samplessss = X_test[y_test == 0].sample(30, random_state=42)\n",
    "\n",
    "# Sample 5 records from Class 1\n",
    "class_1_samplessss = X_test[y_test == 1].sample(30, random_state=42)\n",
    "\n",
    "# Combine the samples\n",
    "X_manual_sampledssd = pd.concat([class_0_samplessss, class_1_samplessss])\n",
    "\n",
    "# Get the corresponding labels\n",
    "y_manual_sampledssd = y_test.loc[X_manual_sampledssd.index]\n",
    "\n",
    "# Save the original (unscaled) test samples for manual checking\n",
    "X_manual_sampledssd.to_csv(\"manual_test_sample_rawdssd.csv\", index=False)\n",
    "y_manual_sampledssd.to_csv(\"manual_test_sample_labelsdssd.csv\", index=False)\n",
    "\n",
    "# Display the sampled data\n",
    "print(\"Sampled Features:\")\n",
    "print(X_manual_sampledssd)\n",
    "print(\"\\nSampled Labels:\")\n",
    "print(y_manual_sampledssd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure indexes of X and y are correctly aligned\n",
    "assert X.index.equals(y.index), \"Mismatch between X and y indexes!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check index alignment for training set\n",
    "assert X_train.index.equals(y_train.index), \"Mismatch in X_train and y_train indexes!\"\n",
    "\n",
    "# Check index alignment for test set\n",
    "assert X_test.index.equals(y_test.index), \"Mismatch in X_test and y_test indexes!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Check test set type\n",
    "\n",
    "print(type(X_test))  # Should be a Pandas DataFrame\n",
    "print(type(y_test))  # Should be a Pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling\n",
    "\n",
    "Since our features might have different scales, we standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Individual Models\n",
    "\n",
    "We train XGBoost, Random Forest, and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:42:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(random_state=42, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "log_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "log_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Individual Models Before Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 XGBoost Performance:\n",
      "Precision: 0.9186\n",
      "Recall: 0.8061\n",
      "F1-Score: 0.8587\n",
      "AUC-PR: 0.8777\n",
      "\n",
      "📌 Random Forest Performance:\n",
      "Precision: 0.9412\n",
      "Recall: 0.8163\n",
      "F1-Score: 0.8743\n",
      "AUC-PR: 0.8734\n",
      "\n",
      "📌 Logistic Regression Performance:\n",
      "Precision: 0.8289\n",
      "Recall: 0.6429\n",
      "F1-Score: 0.7241\n",
      "AUC-PR: 0.7453\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test_scaled, y_test, threshold=0.5):\n",
    "    y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_pr = average_precision_score(y_test, y_probs)\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')\n",
    "    print(f'AUC-PR: {auc_pr:.4f}')\n",
    "\n",
    "print(\"\\n📌 XGBoost Performance:\")    \n",
    "evaluate_model(xgb_model, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\n📌 Random Forest Performance:\")\n",
    "evaluate_model(rf_model, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\n📌 Logistic Regression Performance:\")\n",
    "evaluate_model(log_model, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning\n",
    "\n",
    "We optimize the hyperparameters of the XGBoost model using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:50:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'scale_pos_weight': 1}\n",
      "\n",
      "Best Scores:\n",
      "F1: 0.8605\n",
      "Precision: 0.9538\n",
      "Recall: 0.7996\n",
      "AUC-PR: 0.8495\n"
     ]
    }
   ],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],  \n",
    "    'max_depth': [5, 7],  \n",
    "    'learning_rate': [0.05, 0.1],  \n",
    "    'scale_pos_weight': [1, 5, 10, 15]  # ✅ Adjust for imbalance\n",
    "    #'scale_pos_weight': [10, 25, 50]\n",
    "}\n",
    "\n",
    "\n",
    "# ✅ Define XGBoost model with additional parameters for hyperparameter tuning\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    objective=\"binary:logistic\",   # ✅ Correct for fraud detection\n",
    "    eval_metric=[\"logloss\", \"aucpr\"],  # ✅ LogLoss + AUC-PR for imbalanced data\n",
    "    tree_method=\"hist\",   # ✅ Faster training\n",
    "    verbosity=1,  # ✅ Shows progress logs\n",
    "    random_state=42  \n",
    ")\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    \"F1\": \"f1\",\n",
    "    \"Precision\": \"precision\",\n",
    "    \"Recall\": \"recall\",\n",
    "    \"AUC-PR\": \"average_precision\"  # ✅ Optimizing for AUC-PR\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb_model, param_grid_xgb, cv=3, scoring=scoring_metrics, \n",
    "    n_jobs=-1, refit=\"AUC-PR\"  # ✅ Selects best model based on AUC-PR\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV using the scaled training data\n",
    "grid_xgb.fit(X_train_scaled, y_train)  \n",
    "\n",
    "# Get the best model based on AUC-PR\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"Best Parameters:\", grid_xgb.best_params_)\n",
    "\n",
    "# ✅ Display best scores for all metrics correctly\n",
    "print(\"\\nBest Scores:\")\n",
    "for metric in scoring_metrics.keys():  # Loop over metric names\n",
    "    metric_key = f\"mean_test_{metric}\"  # Correct key in cv_results_\n",
    "    score = grid_xgb.cv_results_[metric_key].max()  # Get the best value for each metric\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall Curve & Optimal Threshold Selection\n",
    "\n",
    "We compute precision-recall scores at different thresholds and select the best one automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best Threshold: 0.470\n",
      "\n",
      "📌 XGBoost (Tuned) Performance:\n",
      "Precision: 0.9022\n",
      "Recall: 0.8469\n",
      "F1-Score: 0.8737\n",
      "AUC-PR: 0.8767\n",
      "\n",
      "📌 Random Forest Performance:\n",
      "Precision: 0.9310\n",
      "Recall: 0.8265\n",
      "F1-Score: 0.8757\n",
      "AUC-PR: 0.8734\n",
      "\n",
      "📌 Logistic Regression Performance:\n",
      "Precision: 0.8312\n",
      "Recall: 0.6531\n",
      "F1-Score: 0.7314\n",
      "AUC-PR: 0.7453\n"
     ]
    }
   ],
   "source": [
    "# ✅ Select the Best Threshold #\n",
    "# ============================ #\n",
    "\n",
    "# Get fraud probabilities\n",
    "y_probs = best_xgb.predict_proba(X_test_scaled)[:, 1]  \n",
    "\n",
    "# Compute precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Compute F1 scores, handling NaN cases\n",
    "f1_scores = np.nan_to_num(2 * (precisions * recalls) / (precisions + recalls))\n",
    "\n",
    "# Select the best threshold\n",
    "best_threshold = thresholds[np.argmax(f1_scores[:-1])]  \n",
    "\n",
    "print(f\"\\n✅ Best Threshold: {best_threshold:.3f}\")\n",
    "\n",
    "# ============================ #\n",
    "# ✅ Evaluate All Models       #\n",
    "# ============================ #\n",
    "\n",
    "print(\"\\n📌 XGBoost (Tuned) Performance:\")\n",
    "evaluate_model(best_xgb, X_test_scaled, y_test, best_threshold)\n",
    "\n",
    "print(\"\\n📌 Random Forest Performance:\")\n",
    "evaluate_model(rf_model, X_test_scaled, y_test, best_threshold)\n",
    "\n",
    "print(\"\\n📌 Logistic Regression Performance:\")\n",
    "evaluate_model(log_model, X_test_scaled, y_test, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall vs. Decision Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Precision-Recall vs Threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precisions[:-1], label=\"Precision\", linestyle=\"--\")\n",
    "plt.plot(thresholds, recalls[:-1], label=\"Recall\")\n",
    "plt.axvline(best_threshold, color='r', linestyle=\":\", label=\"Best Threshold\")  # Mark the best threshold\n",
    "plt.xlabel(\"Decision Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall vs. Decision Threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Final Model Evaluation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 XGBoost (Tuned) Performance (Threshold 0.46982505917549133):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.90      0.85      0.87        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.92      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 XGBoost (Tuned) Performance (Threshold 0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.90      0.83      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.91      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 Random Forest Performance (Threshold 0.46982505917549133):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.93      0.83      0.88        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.91      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 Random Forest Performance (Threshold 0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.94      0.82      0.87        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.91      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 Logistic Regression Performance (Threshold 0.46982505917549133):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.83      0.65      0.73        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.83      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 Logistic Regression Performance (Threshold 0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.83      0.64      0.72        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.91      0.82      0.86     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "🏆 Best Model for Deployment (Default Threshold): Random Forest (F1-Score: 0.8757) ✅\n",
      "🏆 Best Model for Deployment (Best Threshold): Random Forest (F1-Score: 0.8743) ✅\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate models and return key metrics with both thresholds\n",
    "def evaluate_and_print(model, X_test_scaled, y_test, threshold, model_name, is_best_threshold=True):\n",
    "    \"\"\"\n",
    "    This function evaluates the model using a specified threshold (either best or default).\n",
    "    - model: The model to evaluate (e.g., XGBoost, Random Forest, Logistic Regression)\n",
    "    - X_test_scaled: The test data for prediction\n",
    "    - y_test: The true labels for the test data\n",
    "    - threshold: The threshold to apply for classification (best or default)\n",
    "    - model_name: The name of the model (for display)\n",
    "    - is_best_threshold: Whether to use the best threshold or the default threshold (0.5)\n",
    "    \"\"\"\n",
    "    # Get predicted probabilities\n",
    "    y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Apply the appropriate threshold: either best threshold or default (0.5)\n",
    "    threshold_to_use = threshold if is_best_threshold else 0.5\n",
    "    y_pred = (y_probs >= threshold_to_use).astype(int)  # Convert probabilities to binary predictions\n",
    "    \n",
    "    # Get evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n📌 {model_name} Performance (Threshold {threshold_to_use}):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Return F1-score for comparison\n",
    "    return report[\"1\"][\"f1-score\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"XGBoost (Tuned)\": best_xgb,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Logistic Regression\": log_model\n",
    "}\n",
    "\n",
    "# Store F1-scores for model selection with both thresholds\n",
    "f1_scores_default = {}\n",
    "f1_scores_best = {}\n",
    "\n",
    "# Evaluate each model with both the default and best threshold\n",
    "for name, model in models.items():\n",
    "    f1_scores_default[name] = evaluate_and_print(model, X_test_scaled, y_test, best_threshold, name, is_best_threshold=True)  # Using best threshold\n",
    "    f1_scores_best[name] = evaluate_and_print(model, X_test_scaled, y_test, best_threshold, name, is_best_threshold=False)  # Using default threshold\n",
    "\n",
    "# Compare the F1-scores and select the best model based on both thresholds\n",
    "best_model_default_threshold = max(f1_scores_default, key=f1_scores_default.get)\n",
    "best_model_best_threshold = max(f1_scores_best, key=f1_scores_best.get)\n",
    "\n",
    "print(f\"\\n🏆 Best Model for Deployment (Default Threshold): {best_model_default_threshold} (F1-Score: {f1_scores_default[best_model_default_threshold]:.4f}) ✅\")\n",
    "print(f\"🏆 Best Model for Deployment (Best Threshold): {best_model_best_threshold} (F1-Score: {f1_scores_best[best_model_best_threshold]:.4f}) ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 XGBoost (Tuned) Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.90      0.85      0.87        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.92      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.93      0.83      0.88        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.91      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "📌 Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.83      0.65      0.73        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.83      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "\n",
      "🏆 Best Model for Deployment: Random Forest (F1-Score: 0.8757) ✅\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate models and return key metrics\n",
    "def evaluate_and_print(model, X_test_scaled, y_test, threshold, model_name):\n",
    "    # Get predicted probabilities\n",
    "    y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Apply best threshold\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "    # Get evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n📌 {model_name} Performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Return F1-score for comparison\n",
    "    return report[\"1\"][\"f1-score\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"XGBoost (Tuned)\": best_xgb,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Logistic Regression\": log_model\n",
    "}\n",
    "\n",
    "# Store F1-scores for model selection\n",
    "f1_scores = {}\n",
    "\n",
    "# Best threshold found earlier\n",
    "for name, model in models.items():\n",
    "    f1_scores[name] = evaluate_and_print(model, X_test_scaled, y_test, best_threshold, name)\n",
    "\n",
    "# Select the best model based on F1-score\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n🏆 Best Model for Deployment: {best_model_name} (F1-Score: {f1_scores[best_model_name]:.4f}) ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "joblib.dump(xgb_model, 'xgb_model.pkl')\n",
    "joblib.dump(rf_model, 'rf_model.pkl')\n",
    "joblib.dump(log_model, 'log_model.pkl')\n",
    "\n",
    "# Save the best XGBoost model from GridSearchCV\n",
    "joblib.dump(best_xgb, 'best_xgb_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "import json\n",
    "\n",
    "# Convert best_threshold to a standard Python float\n",
    "best_threshold_float = float(best_threshold)\n",
    "\n",
    "# Save the optimal threshold\n",
    "with open('best_threshold.json', 'w') as f:\n",
    "    json.dump({'best_threshold': best_threshold_float}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample data\n",
    "X_manual_sample.to_csv('manual_test_sample.csv', index=False)\n",
    "y_manual_sample.to_csv('manual_test_sample_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained models\n",
    "xgb_model = joblib.load('xgb_model.pkl')\n",
    "rf_model = joblib.load('rf_model.pkl')\n",
    "log_model = joblib.load('log_model.pkl')\n",
    "best_xgb = joblib.load('best_xgb_model.pkl')\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the optimal threshold\n",
    "with open('best_threshold.json', 'r') as f:\n",
    "    best_threshold = json.load(f)['best_threshold']\n",
    "\n",
    "# Use the threshold for predictions\n",
    "y_pred = (y_probs >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sample data\n",
    "X_manual_sample = pd.read_csv('manual_test_sample.csv')\n",
    "y_manual_sample = pd.read_csv('manual_test_sample_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Validation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the sample data\n",
    "X_manual_sample_scaled = scaler.transform(X_manual_sample)\n",
    "\n",
    "# Predict using the best XGBoost model\n",
    "y_probs = best_xgb.predict_proba(X_manual_sample_scaled)[:, 1]\n",
    "y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Create a DataFrame to compare actual and predicted labels\n",
    "results = pd.DataFrame({\n",
    "    'Actual Label': y_manual_sample['Class'],  # Assuming 'Class' is the column name for the actual labels\n",
    "    'Predicted Label': y_pred,\n",
    "    'Predicted Probability': y_probs\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n",
    "# Evaluate the predictions\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "precision = precision_score(y_manual_sample, y_pred)\n",
    "recall = recall_score(y_manual_sample, y_pred)\n",
    "f1 = f1_score(y_manual_sample, y_pred)\n",
    "auc_pr = average_precision_score(y_manual_sample, y_probs)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print(f'AUC-PR: {auc_pr:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_manual_sample, y_pred))\n",
    "\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model\n",
    "best_xgb = joblib.load('best_xgb_model.pkl')\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Load the optimal threshold\n",
    "with open('best_threshold.json', 'r') as f:\n",
    "    best_threshold = json.load(f)['best_threshold']\n",
    "\n",
    "# Load the sample data\n",
    "X_manual_sample = pd.read_csv('manual_test_sample.csv')\n",
    "y_manual_sample = pd.read_csv('manual_test_sample_labels.csv')\n",
    "\n",
    "# Scale the sample data\n",
    "X_manual_sample_scaled = scaler.transform(X_manual_sample)\n",
    "\n",
    "# Generate predicted probabilities\n",
    "y_probs = best_xgb.predict_proba(X_manual_sample_scaled)[:, 1]\n",
    "\n",
    "# Apply the threshold to get binary predictions\n",
    "y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Compare actual and predicted labels\n",
    "results = pd.DataFrame({\n",
    "    'Actual Label': y_manual_sample['Class'],  # Assuming 'Class' is the column name for the actual labels\n",
    "    'Predicted Label': y_pred,\n",
    "    'Predicted Probability': y_probs\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n",
    "# Evaluate the predictions\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "precision = precision_score(y_manual_sample, y_pred)\n",
    "recall = recall_score(y_manual_sample, y_pred)\n",
    "f1 = f1_score(y_manual_sample, y_pred)\n",
    "auc_pr = average_precision_score(y_manual_sample, y_probs)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print(f'AUC-PR: {auc_pr:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_manual_sample, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Define the parameter space for Bayesian Optimization\n",
    "param_space = {\n",
    "    'max_depth': Integer(3, 10),  # Range for max_depth\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),  # Log-uniform range for learning_rate\n",
    "    'n_estimators': Integer(100, 500),  # Range for n_estimators\n",
    "    'subsample': Real(0.5, 1.0),  # Range for subsample\n",
    "    'colsample_bytree': Real(0.5, 1.0),  # Range for colsample_bytree\n",
    "    'gamma': Real(0, 5),  # Range for gamma\n",
    "    'scale_pos_weight': Integer(1, 20)  # Adjust for class imbalance\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:00:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8817\n",
      "Recall: 0.8367\n",
      "F1-Score: 0.8586\n",
      "AUC-PR: 0.8570\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_models = XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Set up Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb_models,\n",
    "    search_spaces=param_space,\n",
    "    scoring='f1',  # Use F1-score for imbalanced datasets\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),  # Cross-validation\n",
    "    n_iter=50,  # Number of iterations (evaluations)\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1,  # Print progress\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian Optimization\n",
    "bayes_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "evaluate_model(bayes_search, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: OrderedDict({'colsample_bytree': 1.0, 'gamma': 0.0, 'learning_rate': 0.29999999999999993, 'max_depth': 3, 'n_estimators': 394, 'scale_pos_weight': 8, 'subsample': 0.5})\n",
      "Precision: 0.8817\n",
      "Recall: 0.8367\n",
      "F1-Score: 0.8586\n",
      "AUC-PR: 0.8570\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.88      0.84      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.94      0.92      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", bayes_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_xgbs = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_pred = best_xgbs.predict(X_test_scaled)\n",
    "y_probs = best_xgbs.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_pr = average_precision_score(y_test, y_probs)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print(f'AUC-PR: {auc_pr:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_xgbs_model.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best XGBoost model from BayesSearchCV\n",
    "joblib.dump(best_xgbs, 'best_xgbs_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
