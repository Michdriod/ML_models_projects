{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Fraud Detection Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: best_xgb_model.pkl\n",
      "Deleted: final_log_model.pkl\n",
      "Deleted: final_rf_model.pkl\n",
      "Deleted: final_threshold.pkl\n",
      "Deleted: final_xgb_model.pkl\n",
      "Deleted: logistic_regression_model.pkl\n",
      "Deleted: random_forest_model.pkl\n",
      "Deleted: scaler.pkl\n",
      "Deleted: xgb_initial_model.pkl\n",
      "Deleted: fraud_predictions_check.xlsx\n",
      "\n",
      "✅ All saved models, scalers, and files have been deleted!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the file types to delete (adjust extensions if needed)\n",
    "files_to_delete = [\"*.pkl\", \"*.csv\", \"*.xlsx\"]  # Adjust as needed\n",
    "\n",
    "# Loop through each file type and delete them\n",
    "for file_type in files_to_delete:\n",
    "    for file in glob.glob(file_type):  # Find all matching files\n",
    "        try:\n",
    "            os.remove(file)  # Delete the file\n",
    "            print(f\"Deleted: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {e}\")\n",
    "\n",
    "print(\"\\n✅ All saved models, scalers, and files have been deleted!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model Training & Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# Model Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Ensemble Learning\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# analyze the impact of each feature on the model's decision-making process.\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Explore the Dataset (Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "credit_card_fraud = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Downloads\\\\creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "\n",
    "credit_card_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the last few rows\n",
    "\n",
    "credit_card_fraud.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset shape\n",
    "\n",
    "credit_card_fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution\n",
    "\n",
    "credit_card_fraud[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# exploring the datatype of each column\n",
    "\n",
    "credit_card_fraud.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "credit_card_fraud.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check basic statistics of the dataset\n",
    "\n",
    "credit_card_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance Ratio: 577.8760\n"
     ]
    }
   ],
   "source": [
    "# imbalance ratio of fruad cases and normal cases\n",
    "\n",
    "fraud_cases = credit_card_fraud[credit_card_fraud[\"Class\"] == 1]\n",
    "normal_cases = credit_card_fraud[credit_card_fraud[\"Class\"] == 0]\n",
    "\n",
    "imbalance_ratio = len(normal_cases) / len(fraud_cases)\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Features & Target\n",
    "\n",
    "We separate the features from the target variable and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit_card_fraud.drop(columns=['Class'])  # Adjust column name accordingly\n",
    "y = credit_card_fraud['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10 records from the test set (before scaling)\n",
    "sample_indices = X_test.sample(10, random_state=42).index\n",
    "X_manual_sample = X_test.loc[sample_indices]\n",
    "y_manual_sample = y_test.loc[sample_indices]\n",
    "\n",
    "# Save the original (unscaled) test samples for manual checking\n",
    "X_manual_sample.to_csv(\"manual_test_sample_raw.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure indexes of X and y are correctly aligned\n",
    "assert X.index.equals(y.index), \"Mismatch between X and y indexes!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X and y are Pandas DataFrame/Series\n",
    "X = pd.DataFrame(X)  # If X is a NumPy array, convert it to DataFrame\n",
    "y = pd.Series(y)  # If y is a NumPy array, convert it to Series\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Now check index alignment\n",
    "assert X_train.index.equals(y_train.index), \"Mismatch in X_train and y_train indexes!\"\n",
    "assert X_test.index.equals(y_test.index), \"Mismatch in X_test and y_test indexes!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample index\n",
    "sample_idx = X_train.index[0]  \n",
    "\n",
    "# Verify feature-label consistency\n",
    "print(\"Features:\", X_train.loc[sample_idx])\n",
    "print(\"Label:\", y_train.loc[sample_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features = X_test.iloc[0].values.reshape(1, -1)  # Ensure correct shape\n",
    "prediction = model.predict(sample_features)\n",
    "print(\"Predicted Label:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0 in X_test.index)  # True if present, False if missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = X_test.index[0]  # Pick the first available index\n",
    "sample_features = X_test.loc[sample_index].values.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "y_train, y_test = y_train.reset_index(drop=True), y_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check index alignment for training set\n",
    "assert X_train.index.equals(y_train.index), \"Mismatch in X_train and y_train indexes!\"\n",
    "\n",
    "# Check index alignment for test set\n",
    "assert X_test.index.equals(y_test.index), \"Mismatch in X_test and y_test indexes!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test, columns=feature_names)  # Ensure you have feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_test))  # Should be a Pandas DataFrame\n",
    "print(type(y_test))  # Should be a Pandas Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset index for consistency\n",
    "# X_test = X_test.reset_index(drop=True)\n",
    "# y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Now, select fraud and non-fraud indices\n",
    "fraud_indices = y_test[y_test == 1].index.tolist()\n",
    "non_fraud_indices = y_test[y_test == 0].index.tolist()\n",
    "\n",
    "# Randomly select 10 fraud and non-fraud samples\n",
    "num_fraud = min(10, len(fraud_indices))\n",
    "num_non_fraud = min(10, len(non_fraud_indices))\n",
    "\n",
    "random_fraud = np.random.choice(fraud_indices, size=num_fraud, replace=False).tolist()\n",
    "random_non_fraud = np.random.choice(non_fraud_indices, size=num_non_fraud, replace=False).tolist()\n",
    "\n",
    "# Combine indices\n",
    "random_indices = random_fraud + random_non_fraud  \n",
    "\n",
    "# Select samples\n",
    "X_test_samples = X_test.loc[random_indices]\n",
    "y_test_samples = y_test.loc[random_indices]\n",
    "\n",
    "# Display results\n",
    "print(\"Selected X_test Samples:\\n\", X_test_samples)\n",
    "print(\"\\nActual y_test Labels:\\n\", y_test_samples.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get fraud and non-fraud indices\n",
    "# fraud_indices = y_test[y_test == 1].index\n",
    "# non_fraud_indices = y_test[y_test == 0].index\n",
    "\n",
    "# # Determine how many fraud cases exist\n",
    "# num_fraud = min(10, len(fraud_indices))  # Ensure we don't exceed available fraud cases\n",
    "# num_non_fraud = min(10, len(non_fraud_indices))  # Same for non-fraud\n",
    "\n",
    "# # Select random samples\n",
    "# random_fraud = np.random.choice(fraud_indices, size=num_fraud, replace=False) if num_fraud > 0 else []\n",
    "# random_non_fraud = np.random.choice(non_fraud_indices, size=num_non_fraud, replace=False)\n",
    "\n",
    "# # Combine indices\n",
    "# random_indices = np.concatenate([random_fraud, random_non_fraud])\n",
    "\n",
    "# # Select corresponding X_test and y_test samples\n",
    "# X_test_samples = X_test.loc[random_indices]\n",
    "# y_test_samples = y_test.loc[random_indices]\n",
    "\n",
    "# # Display results\n",
    "# print(\"Selected X_test Samples:\\n\", X_test_samples)\n",
    "# print(\"\\nActual y_test Labels:\\n\", y_test_samples.value_counts())  # Show fraud (1) and non-fraud (0) count\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure X_test is a DataFrame with the same index as y_test\n",
    "if isinstance(X_test, np.ndarray):  \n",
    "    X_test = pd.DataFrame(X_test, index=y_test.index, columns=feature_names)  # Ensure the index matches y_test\n",
    "\n",
    "# Get fraud and non-fraud indices\n",
    "fraud_indices = y_test[y_test == 1].index\n",
    "non_fraud_indices = y_test[y_test == 0].index\n",
    "\n",
    "# Determine how many fraud cases exist\n",
    "num_fraud = min(10, len(fraud_indices))  \n",
    "num_non_fraud = min(10, len(non_fraud_indices))  \n",
    "\n",
    "# Select random samples\n",
    "random_fraud = np.random.choice(fraud_indices, size=num_fraud, replace=False) if num_fraud > 0 else []\n",
    "random_non_fraud = np.random.choice(non_fraud_indices, size=num_non_fraud, replace=False)\n",
    "\n",
    "# Convert NumPy arrays to lists\n",
    "random_fraud = list(random_fraud)\n",
    "random_non_fraud = list(random_non_fraud)\n",
    "\n",
    "# Combine indices\n",
    "random_indices = random_fraud + random_non_fraud  \n",
    "\n",
    "# Select corresponding X_test and y_test samples\n",
    "X_test_samples = X_test.loc[random_indices]  # Now, indices should exist in X_test\n",
    "y_test_samples = y_test.loc[random_indices]\n",
    "\n",
    "# Display results\n",
    "print(\"Selected X_test Samples:\\n\", X_test_samples)\n",
    "print(\"\\nActual y_test Labels:\\n\", y_test_samples.value_counts())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling\n",
    "\n",
    "Since our features might have different scales, we standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Individual Models\n",
    "\n",
    "We train XGBoost, Random Forest, and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "log_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Individual Models Before Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_pr = average_precision_score(y_test, y_probs)\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')\n",
    "    print(f'AUC-PR: {auc_pr:.4f}')\n",
    "\n",
    "print(\"\\n📌 XGBoost Performance:\")    \n",
    "evaluate_model(xgb_model, X_test, y_test)\n",
    "\n",
    "print(\"\\n📌 Random Forest Performance:\")\n",
    "evaluate_model(rf_model, X_test, y_test)\n",
    "\n",
    "print(\"\\n📌 Logistic Regression Performance:\")\n",
    "evaluate_model(log_model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning\n",
    "\n",
    "We optimize the hyperparameters of the XGBoost model using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "# param_grid_xgb = {\n",
    "#     'n_estimators': [50, 100, 200],  \n",
    "#     'max_depth': [3, 5, 7],  \n",
    "#     'learning_rate': [0.01, 0.1, 0.2]  \n",
    "# }\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],  \n",
    "    'max_depth': [5, 7],  \n",
    "    'learning_rate': [0.05, 0.1],  \n",
    "    'scale_pos_weight': [1, 10, 25, 50]  # ✅ Adjust for imbalance\n",
    "    # 'scale_pos_weight': [10, 25, 50]\n",
    "}\n",
    "\n",
    "\n",
    "# ✅ Define XGBoost model with additional parameters for hyperparameter tuning\n",
    "xgb_models = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    objective=\"binary:logistic\",   # ✅ Correct for fraud detection\n",
    "    eval_metric=[\"logloss\", \"aucpr\"],  # ✅ LogLoss + AUC-PR for imbalanced data\n",
    "    tree_method=\"hist\",   # ✅ Faster training\n",
    "    verbosity=1,  # ✅ Shows progress logs\n",
    "    random_state=42  \n",
    ")\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    \"F1\": \"f1\",\n",
    "    \"Precision\": \"precision\",\n",
    "    \"Recall\": \"recall\",\n",
    "    \"AUC-PR\": \"average_precision\"  # ✅ Optimizing for AUC-PR\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb_models, param_grid_xgb, cv=3, scoring=scoring_metrics, \n",
    "    n_jobs=-1, refit=\"AUC-PR\"  # ✅ Selects best model based on AUC-PR\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV using the scaled training data\n",
    "grid_xgb.fit(X_train, y_train)  # ✅ Use X_train_scaled\n",
    "\n",
    "# Get the best model based on AUC-PR\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"Best Parameters:\", grid_xgb.best_params_)\n",
    "\n",
    "# ✅ Display best scores for all metrics correctly\n",
    "print(\"\\nBest Scores:\")\n",
    "for metric in scoring_metrics.keys():  # Loop over metric names\n",
    "    metric_key = f\"mean_test_{metric}\"  # Correct key in cv_results_\n",
    "    score = grid_xgb.cv_results_[metric_key].max()  # Get the best value for each metric\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall Curve & Optimal Threshold Selection\n",
    "\n",
    "We compute precision-recall scores at different thresholds and select the best one automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-Tune scale_pos_weight Further\n",
    "# best_xgb.set_params(scale_pos_weight=5)  # Try 5, 10, or 15\n",
    "# best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get fraud probabilities\n",
    "y_probs = best_xgb.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Compute precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Compute F1 scores, handling NaN cases\n",
    "f1_scores = np.nan_to_num(2 * (precisions * recalls) / (precisions + recalls))\n",
    "\n",
    "# Ensure threshold index is correctly aligned\n",
    "best_threshold = thresholds[np.argmax(f1_scores[:-1])]  # Ensure no index mismatch\n",
    "\n",
    "print(f\"\\n✅ Best Threshold: {best_threshold:.3f}\")\n",
    "\n",
    "# Evaluate Models with Best Threshold\n",
    "print(\"\\n📌 XGBoost Performance:\")\n",
    "evaluate_model(best_xgb, X_test, y_test, best_threshold)\n",
    "\n",
    "print(\"\\n📌 XGBoosts Performance:\")\n",
    "evaluate_model(xgb_model, X_test, y_test, best_threshold)\n",
    "\n",
    "print(\"\\n📌 Random Forest Performance:\")\n",
    "evaluate_model(rf_model, X_test, y_test, best_threshold)\n",
    "\n",
    "print(\"\\n📌 Logistic Regression Performance:\")\n",
    "evaluate_model(log_model, X_test, y_test, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Best Threshold to Your Initial XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fraud probabilities from your initial XGBoost model\n",
    "y_probs_initial = xgb_model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Apply the best threshold from the tuned model\n",
    "y_pred_initial = (y_probs_initial >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate initial model with the optimized threshold\n",
    "print(\"\\n📌 Initial XGBoost Model Performance with Best Threshold:\")\n",
    "print(classification_report(y_test, y_pred_initial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot Precision-Recall vs Threshold\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(best_threshold, precisions[:-1], label=\"Precision\", linestyle=\"--\")\n",
    "# plt.plot(best_threshold, recalls[:-1], label=\"Recall\")\n",
    "# plt.xlabel(\"best_threshold\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.title(\"Precision-Recall vs. Decision Threshold\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Plot Precision-Recall vs Threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precisions[:-1], label=\"Precision\", linestyle=\"--\")\n",
    "plt.plot(thresholds, recalls[:-1], label=\"Recall\")\n",
    "plt.axvline(best_threshold, color='r', linestyle=\":\", label=\"Best Threshold\")  # Mark the best threshold\n",
    "plt.xlabel(\"Decision Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall vs. Decision Threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an Ensemble Model (XGBoost + RF + Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble = VotingClassifier(estimators=[('xgb', best_xgb), ('rf', rf_model), ('log', log_model)], voting='soft')\n",
    "# ensemble.fit(X_train, y_train)\n",
    "# evaluate_model(ensemble, X_test, y_test, best_threshold)\n",
    "\n",
    "# # ensemble_model = VotingClassifier(\n",
    "# #     estimators=[\n",
    "# #         ('xgb', best_xgb),\n",
    "# #         ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "# #         ('log', LogisticRegression())\n",
    "# #     ],\n",
    "# #     voting='soft',  # Probability-based voting\n",
    "# #     weights=[3, 2, 1]  # Giving more weight to XGBoost\n",
    "# # )\n",
    "# # ensemble_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔹 SHAP Analysis for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model (Tree-Based)\n",
    "explainer_xgb = shap.Explainer(best_xgb)\n",
    "shap_values_xgb = explainer_xgb(X_test)\n",
    "shap.summary_plot(shap_values_xgb, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoosts Model (Tree-Based)\n",
    "explainer_xgb = shap.Explainer(xgb_model)\n",
    "shap_values_xgb_ = explainer_xgb(X_test)\n",
    "shap.summary_plot(shap_values_xgb_, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Model (Tree-Based)\n",
    "# explainer_rf = shap.TreeExplainer(rf_model)  # Use TreeExplainer for RF\n",
    "# shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "# shap.summary_plot(shap_values_rf[1], X_test)  # Use class index [1] for fraud cases\n",
    "# shap.summary_plot(shap_values_rf[0], X_test)\n",
    "\n",
    "\n",
    "# Use the proper SHAP explainer for Random Forest\n",
    "explainer_rf = shap.TreeExplainer(rf_model, feature_perturbation=\"tree_path_dependent\")\n",
    "shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "\n",
    "# Check if SHAP returns a list or array\n",
    "if isinstance(shap_values_rf, list):  # Multi-class case\n",
    "    shap.summary_plot(shap_values_rf[1], X_test)  # Fraud class\n",
    "else:  # Single output case\n",
    "    shap.summary_plot(shap_values_rf, X_test)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(type(shap_values_rf))  # Check if it's a list or NumPy array\n",
    "\n",
    "\n",
    "\n",
    "print(np.shape(shap_values_rf))  # Print shape of SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression Model (Linear)\n",
    "explainer_log = shap.LinearExplainer(log_model, X_train)  # Use LinearExplainer for LR\n",
    "shap_values_log = explainer_log.shap_values(X_test)\n",
    "shap.summary_plot(shap_values_log, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Final Model Evaluation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models and return key metrics\n",
    "def evaluate_and_print(model, X_test, y_test, threshold, model_name):\n",
    "    # Get predicted probabilities\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Apply best threshold\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "    # Get evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n📌 {model_name} Performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Return F1-score for comparison\n",
    "    return report[\"1\"][\"f1-score\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"XGBoost (Tuned)\": best_xgb,\n",
    "    \"XGBoost (Initial)\": xgb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Logistic Regression\": log_model\n",
    "}\n",
    "\n",
    "# Store F1-scores for model selection\n",
    "f1_scores = {}\n",
    "\n",
    "# Best threshold found earlier\n",
    "for name, model in models.items():\n",
    "    f1_scores[name] = evaluate_and_print(model, X_test, y_test, best_threshold, name)\n",
    "\n",
    "# Select the best model based on F1-score\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n🏆 Best Model for Deployment: {best_model_name} (F1-Score: {f1_scores[best_model_name]:.4f}) ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models and return key metrics with both thresholds\n",
    "def evaluate_and_print(model, X_test_scaled, y_test, threshold, model_name, is_best_threshold=True):\n",
    "    \"\"\"\n",
    "    This function evaluates the model using a specified threshold (either best or default).\n",
    "    - model: The model to evaluate (e.g., XGBoost, Random Forest, Logistic Regression)\n",
    "    - X_test_scaled: The test data for prediction\n",
    "    - y_test: The true labels for the test data\n",
    "    - threshold: The threshold to apply for classification (best or default)\n",
    "    - model_name: The name of the model (for display)\n",
    "    - is_best_threshold: Whether to use the best threshold or the default threshold (0.5)\n",
    "    \"\"\"\n",
    "    # Get predicted probabilities\n",
    "    y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Apply the appropriate threshold: either best threshold or default (0.5)\n",
    "    threshold_to_use = threshold if is_best_threshold else 0.5\n",
    "    y_pred = (y_probs >= threshold_to_use).astype(int)  # Convert probabilities to binary predictions\n",
    "    \n",
    "    # Get evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n📌 {model_name} Performance (Threshold {threshold_to_use}):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Return F1-score for comparison\n",
    "    return report[\"1\"][\"f1-score\"]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"XGBoost (Tuned)\": best_xgb,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Logistic Regression\": log_model\n",
    "}\n",
    "\n",
    "# Store F1-scores for model selection with both thresholds\n",
    "f1_scores_default = {}\n",
    "f1_scores_best = {}\n",
    "\n",
    "# Evaluate each model with both the default and best threshold\n",
    "for name, model in models.items():\n",
    "    f1_scores_default[name] = evaluate_and_print(model, X_test_scaled, y_test, best_threshold, name, is_best_threshold=True)  # Using best threshold\n",
    "    f1_scores_best[name] = evaluate_and_print(model, X_test_scaled, y_test, best_threshold, name, is_best_threshold=False)  # Using default threshold\n",
    "\n",
    "# Compare the F1-scores and select the best model based on both thresholds\n",
    "best_model_default_threshold = max(f1_scores_default, key=f1_scores_default.get)\n",
    "best_model_best_threshold = max(f1_scores_best, key=f1_scores_best.get)\n",
    "\n",
    "print(f\"\\n🏆 Best Model for Deployment (Default Threshold): {best_model_default_threshold} (F1-Score: {f1_scores_default[best_model_default_threshold]:.4f}) ✅\")\n",
    "print(f\"🏆 Best Model for Deployment (Best Threshold): {best_model_best_threshold} (F1-Score: {f1_scores_best[best_model_best_threshold]:.4f}) ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "joblib.dump(best_xgb, \"best_xgb_model.pkl\")\n",
    "joblib.dump(xgb_model, \"xgb_initial_model.pkl\")\n",
    "joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "joblib.dump(log_model, \"logistic_regression_model.pkl\")\n",
    "\n",
    "# Save the fitted scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Convert best_threshold to a standard Python float before saving\n",
    "best_threshold = float(best_threshold)  # Ensures JSON compatibility\n",
    "\n",
    "# Save best threshold\n",
    "with open(\"best_threshold.json\", \"w\") as f:\n",
    "    json.dump({\"best_threshold\": best_threshold}, f)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "with open(\"feature_names.json\", \"w\") as f:\n",
    "    json.dump(feature_names, f)\n",
    "\n",
    "print(\"All models, scaler, best threshold, and feature names have been successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Check Process for model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Files in my Directory(If models are saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the current directory\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Check the Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "loaded_best_xgb = joblib.load(\"best_xgb_model.pkl\")\n",
    "loaded_xgb = joblib.load(\"xgb_initial_model.pkl\")\n",
    "loaded_rf = joblib.load(\"random_forest_model.pkl\")\n",
    "loaded_log = joblib.load(\"logistic_regression_model.pkl\")\n",
    "\n",
    "# Load the scaler\n",
    "loaded_scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Load the best threshold\n",
    "with open(\"best_threshold.json\", \"r\") as f:\n",
    "    best_threshold_data = json.load(f)\n",
    "    loaded_best_threshold = best_threshold_data[\"best_threshold\"]\n",
    "\n",
    "# Load feature names\n",
    "with open(\"feature_names.json\", \"r\") as f:\n",
    "    loaded_feature_names = json.load(f)\n",
    "\n",
    "# Print confirmations\n",
    "print(\"✅ Models and scaler loaded successfully!\")\n",
    "print(\"✅ Best threshold:\", loaded_best_threshold)\n",
    "print(\"✅ Feature names:\", loaded_feature_names[:5], \"...\")  # Print first 5 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to evaluate models and show actual vs. predicted values\n",
    "# def check_predictions(model, X_test_samples, y_test_samples, best_threshold, model_name):\n",
    "#     # Get predicted probabilities\n",
    "#     y_probs = model.predict_proba(X_test_samples)[:, 1]\n",
    "\n",
    "#     # Apply best threshold\n",
    "#     y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "#     # Return predicted values\n",
    "#     return y_pred\n",
    "\n",
    "# # Define models to check\n",
    "# models = {\n",
    "#     \"XGBoost (Tuned)\": best_xgb,\n",
    "#     \"XGBoost (Initial)\": xgb_model,\n",
    "#     \"Random Forest\": rf_model,\n",
    "#     \"Logistic Regression\": log_model\n",
    "# }\n",
    "\n",
    "# # Store predictions\n",
    "# predictions = {}\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     predictions[name] = check_predictions(model, X_test_samples, y_test_samples, best_threshold, name)\n",
    "\n",
    "# # Create results DataFrame\n",
    "# results_df = pd.DataFrame({\n",
    "#     \"Actual\": y_test_samples.values\n",
    "# })\n",
    "\n",
    "# # Add model predictions\n",
    "# for name, preds in predictions.items():\n",
    "#     results_df[name] = preds\n",
    "\n",
    "# # Add a column to check if the predictions were correct for each model\n",
    "# for name in models.keys():\n",
    "#     results_df[f\"{name} Correct?\"] = (results_df[\"Actual\"] == results_df[name]).astype(int)\n",
    "\n",
    "# # Display results before saving\n",
    "# print(results_df)\n",
    "\n",
    "# # Save to Excel if needed\n",
    "# output_file = \"fraud_predictions_check.xlsx\"\n",
    "# results_df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(f\"\\n✅ Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models and show actual vs. predicted values\n",
    "def check_predictions(model, X_test_samples, y_test_samples, best_threshold, model_name, scaler):\n",
    "    # Apply the same scaling transformation used during training\n",
    "    X_test_scaled = scaler.transform(X_test_samples)\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Apply best threshold\n",
    "    y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "    # Return predicted values\n",
    "    return y_pred\n",
    "\n",
    "# Load the pre-trained scaler\n",
    "loaded_scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Define models to check\n",
    "models = {\n",
    "    \"XGBoost (Tuned)\": best_xgb,\n",
    "    \"XGBoost (Initial)\": xgb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Logistic Regression\": log_model\n",
    "}\n",
    "\n",
    "# Store predictions\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions[name] = check_predictions(model, X_test_samples, y_test_samples, best_threshold, name, loaded_scaler)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": y_test_samples.values\n",
    "})\n",
    "\n",
    "# Add model predictions\n",
    "for name, preds in predictions.items():\n",
    "    results_df[name] = preds\n",
    "\n",
    "# Add a column to check if the predictions were correct for each model\n",
    "for name in models.keys():\n",
    "    results_df[f\"{name} Correct?\"] = (results_df[\"Actual\"] == results_df[name]).astype(int)\n",
    "\n",
    "# Display results before saving\n",
    "print(results_df)\n",
    "\n",
    "# Save to Excel if needed\n",
    "output_file = \"fraud_predictions_check.xlsx\"\n",
    "results_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Function to evaluate models and show actual vs. predicted values\n",
    "# def check_predictions(model, X_test_samples):\n",
    "#     # Get predicted probabilities\n",
    "#     y_probs = model.predict_proba(X_test_samples)[:, 1]\n",
    "\n",
    "#     # Apply best threshold\n",
    "#     y_pred = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "#     # Return predicted values\n",
    "#     return y_pred\n",
    "\n",
    "# # Define models to check\n",
    "# models = {\n",
    "#     \"XGBoost (Tuned)\": best_xgb,\n",
    "#     \"XGBoost (Initial)\": xgb_model,\n",
    "#     \"Random Forest\": rf_model,\n",
    "#     \"Logistic Regression\": log_model\n",
    "# }\n",
    "\n",
    "# # Store predictions\n",
    "# predictions = {}\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     predictions[name] = check_predictions(model, X_test_samples)\n",
    "\n",
    "# # Create results DataFrame\n",
    "# results_df = pd.DataFrame({\n",
    "#     \"Actual\": y_test_samples.values\n",
    "# })\n",
    "\n",
    "# # Add model predictions\n",
    "# for name, preds in predictions.items():\n",
    "#     results_df[name] = preds\n",
    "\n",
    "# # Add a column to check if the predictions were correct for each model\n",
    "# for name in models.keys():\n",
    "#     results_df[f\"{name} Correct?\"] = (results_df[\"Actual\"] == results_df[name]).astype(int)\n",
    "\n",
    "# # Display results before saving\n",
    "# print(results_df.head())  # Show only the first few rows for better readability\n",
    "\n",
    "# # Save to Excel\n",
    "# output_file = \"fraud_predictions_check.xlsx\"\n",
    "# results_df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(f\"\\n✅ Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = results_df[results_df[\"Actual\"] != results_df[\"XGBoost (Tuned)\"]]\n",
    "print(misclassified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_probs = best_xgb.predict_proba(X_test_samples.iloc[misclassified.index])[:, 1]\n",
    "misclassified[\"Predicted Probabilities\"] = misclassified_probs\n",
    "print(misclassified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold = 0.8\n",
    "new_preds = (best_xgb.predict_proba(X_test_samples)[:, 1] >= new_threshold).astype(int)\n",
    "\n",
    "# Check new misclassified cases\n",
    "new_misclassified = results_df[results_df[\"Actual\"] != new_preds]\n",
    "print(new_misclassified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test_samples Index:\\n\", X_test_samples.index[:10])  # Show first 10 indices\n",
    "print(\"results_df Index:\\n\", results_df.index[:10])  # Show first 10 indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.set_index(X_test_samples.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_fraud_cases = X_test_samples.loc[\n",
    "    (results_df[\"Actual\"] == 1) & (results_df[\"XGBoost (Tuned) Correct?\"] == 1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex results_df to match X_test_samples\n",
    "aligned_results_df = results_df.loc[X_test_samples.index]\n",
    "\n",
    "# Now apply the boolean indexing with aligned indices\n",
    "correct_fraud_cases = X_test_samples.loc[\n",
    "    (aligned_results_df[\"Actual\"] == 1) & (aligned_results_df[\"XGBoost (Tuned) Correct?\"] == 1)\n",
    "]\n",
    "\n",
    "# Check results\n",
    "print(correct_fraud_cases.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified features using .iloc (by position)\n",
    "misclassified_features = X_test_samples.iloc[misclassified.index]\n",
    "\n",
    "# Get correctly classified fraud cases using .loc (boolean indexing)\n",
    "correct_fraud_cases = X_test_samples.loc[\n",
    "    (results_df[\"Actual\"] == 1) & (results_df[\"XGBoost (Tuned) Correct?\"] == 1)\n",
    "]\n",
    "\n",
    "# Compare statistics\n",
    "print(\"Misclassified Fraud Case Stats:\\n\", misclassified_features.describe())\n",
    "print(\"\\nCorrectly Classified Fraud Case Stats:\\n\", correct_fraud_cases.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correctly Classified Fraud Cases:\\n\", correct_fraud_cases)\n",
    "print(\"Shape:\", correct_fraud_cases.shape)  # Check how many cases were found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display misclassified fraud cases\n",
    "print(\"Misclassified Fraud Cases:\\n\", misclassified_features)\n",
    "\n",
    "# Display statistics of misclassified fraud cases\n",
    "print(\"\\nMisclassified Fraud Case Stats:\\n\", misclassified_features.describe())\n",
    "\n",
    "# Compare with correctly classified fraud cases\n",
    "print(\"\\nCorrectly Classified Fraud Case Stats:\\n\", correct_fraud_cases.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions\n",
    "misclassified_probs = model.predict_proba(misclassified_features)\n",
    "\n",
    "# Display probability scores\n",
    "print(\"Misclassified Case Probabilities:\\n\", misclassified_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold = 0.3  # Adjust this value\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Get fraud probabilities\n",
    "y_pred_new = (y_probs >= new_threshold).astype(int)\n",
    "\n",
    "# Evaluate new threshold\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get feature importances (works for tree-based models like XGBoost or RF)\n",
    "feature_importances = model.feature_importances_\n",
    "features = X_test.columns\n",
    "\n",
    "# Sort and plot top features\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features[sorted_idx][:10], feature_importances[sorted_idx][:10])\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])  # Compute weight\n",
    "model = XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "model = XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# Get feature importances (only for tree-based models)\n",
    "feature_importances = model.feature_importances_\n",
    "features = X_test.columns\n",
    "\n",
    "# Sort and plot top features\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features[sorted_idx][:10], feature_importances[sorted_idx][:10])\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "selector = SelectFromModel(model, threshold=\"median\", prefit=True)\n",
    "X_selected = selector.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Amount_per_V14\"] = X_train[\"Amount\"] / (X_train[\"V14\"] + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(scale_pos_weight=scale_pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "try:\n",
    "    y_pred_new = (model.predict_proba(X_test)[:, 1] >= 0.3).astype(int)\n",
    "except NotFittedError:\n",
    "    print(\"Model is not trained! Retraining now...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_new = (model.predict_proba(X_test)[:, 1] >= 0.3).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold = 0.3  # Instead of 0.5\n",
    "y_pred_new = (model.predict_proba(X_test)[:, 1] >= new_threshold).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
